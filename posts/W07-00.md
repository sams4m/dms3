---
title: ðŸ’¥ Web Audio API & Responding to Text
published_at: 2025-04-17
snippet: Week 07 00
disable_html_sanitization: true
allow_math: true
---

<style>
  @import url('https://fonts.googleapis.com/css2?family=Cutive+Mono&display=swap');
  @import url('https://use.typekit.net/jyw5vxq.css');

h1, h3, h4, p, pre, ul, li, .notranslate {
  /* font-family: "Cutive Mono", monospace;
  font-weight: 700;
  font-style: normal; */

  font-family: "prestige-elite-std", monospace;
  font-weight: 600;
  font-style: normal;
  color:#CEB5D4;
}

 .text-gray-500, .markdown-body blockquote {color:#E872B0}
 .markdown-body {background-color:#102B53;}
  html {background-color:#102B53;}
  h1 {; font-weight: 800;}
  p, pre, ul {color:#7D9FC0;}
  .markdown-body a {color:#4E7AB1; text-decoration:underline;}

  .notranslate, text {
    color: #102B53;
    font-weight: 800;
  }


</style>

---

# Homework

### 1. how will you make the sound design for your AT2 function in a chaotic aesthetic register?Â  What does it mean to be chaotic in the sonic domain? In your discussion, please make reference to:

> Effective complexity, paying particular attention to:
>
> - structure - whatÂ *structures*Â our perception of sound?
> - noise - what differentiatesÂ noiseÂ fromÂ sound, or music?
> - voice - what makesÂ *voice*Â a separate category of sound?

> The role of de-familiarisation (as per Natalie Loveless' talkÂ [From Relational to Ecological Form](https://youtu.be/whzD1EPBVLk))

> Three examples, which can (but don't have to) be taken from the following list:
>
> - MyNoise Generators:Â [Jungle](https://mynoise.net/NoiseMachines/jungleNoiseGenerator.php)Â /Â [Water](https://mynoise.net/NoiseMachines/healingWaterSoundscapeGenerator.php)Â /Â [Fire](https://mynoise.net/NoiseMachines/fireNoiseGenerator.php)Â /Â [Thunder](https://mynoise.net/NoiseMachines/thunderNoiseGenerator.php)Â /Â [Storm](https://mynoise.net/NoiseMachines/stormSoundGenerator.php)
> - [Pink](https://dood.al/pinktrombone)
> - [How to Kill a Zombie.](https://lcld.xyz/240831_how_to_kill)

Effective complexity in itself is a range balance between high structure and high randomness. In the case of sound I'd say that is a state between fine-tuned wavelengths vs interference to this wavelength. Kind of like when adjusting a radio and you're trying to find the right wavelength to listen into the channel that you want you'll often encounter a lot of 'noise' before getting the right wavelength and you can hear the voice/sound a lot more clearly. With this understanding it could be said that noise is the 'random' and sound, the 'clean' wavelength is the 'structured'. Bringing this back to effective complexity, I'd say effective complexity in the sonic domain is a point of intersection between noise and sound. When effectively wielded it could be translated into music. Where, perhaps a genre like pop is a universal mid point between the two, but something like ballads would lean more towards structured and heavy metal leaning more towards noise.

The concept of de-familiarisation is a technique which presents familiar things in unfamiliar ways. This is to enhance the audience's perception and create new awareness of the subject. In this context, this would mean manipulating something familiar to create a fresh experience. In this sense, this concept could be wielded in a way which ensues chaos in the sonic domain.

The Jungle Noise Generator, which separates the jungle noises into the different categories, sub-bass, low-bass, bass, high-bass, low-mids, mids, high-mids, low-treble, treble, high-treble, and allows the user to adjust these categories in a way that defamiliarises the jungle sound. Through this process, it also in a way creates 'noise' even though it's technically just the different registers within the jungle sound itself.

Unlike the Jungle Noise Generator, the storm one categorises the sound track by the different parts and pairings that create the storm, with the broader categories being wind, rain & distant thunder. But likewise provides the user with a new and chaotic experience of "storm noise". (Though storm noise is already quite chaotic due to the unpredictable nature of the wind, rain and thunder)

'Pink' is similar in the way that it takes a very mundane sound, the human mouth, and separates it into a few key categories (structure), pitch, tongue location, voice (which is further separated into wobbly and 'always voice') though ultimately categorised by the parts of the mouth. Which the user manipulates via a click interaction on the visuals and affords a new experience of the sound. In this way, 'Pink' also creates a potential for chaos to ensue as the different categories are wielded by the user.

In relation to Assignment 2, I think that the sound that I've incorporated is already quite chaotic in nature, like the storm sounds for the storm noise generator. And in a similar nature, I think I could further enhance the sound experience by following the concept of de-familiarisation. Manipulating the sound to give it new context. I'm thinking to use a click interaction that will affect the volume at which the sound plays out to give it a de-centering experience. Kind of like a doppler effect but make it even more trippy to really effect the sense of grounding.

### 2. Implement an interactive sound design experiment in your blog.

> - you will need to get some form of user gesture for Web Audio API to run.
> - you can use a library, such asÂ [Tone.js](https://tonejs.github.io/)Â orÂ [Pizzicato.js](https://alemangui.github.io/pizzicato), or you can useÂ [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API), orÂ [AudioWorklet](https://developer.mozilla.org/en-US/docs/Web/API/AudioWorklet).
> - use a sinusoid to push your sound design into chaos and back every 24 seconds.
> - explain your code with the help of syntax-highlighted, commented code blocks
> - evaluate the success of your experiment with reference to your discussion in task 1

<div id="sound" style="margin-bottom: 5%;"> </div>

<script id="script">
// get and format div element
const div  = document.getElementById ('sound');
div.width  = div.parentNode.scrollWidth;
div.style.height     = `${ div.width * 9 / 16 }px`;
div.style.backgroundColor = 'pink';

// get and suspend audio context
const audioContext = new AudioContext();
audioContext.suspend();

// define an async click handler function 
async function init_audio() {
  // wait for audio context to resume
  await audioContext.resume();
}

// OSCILLATOR NODE
// store a new oscillator node in a variable
const oscNode = audioContext.createOscillator();
// defining oscillator node type
oscNode.type = 'sawtooth';

// frequency
oscNode.frequency.value = 330;

// store new gain node in a variable
const ampNode = audioContext.createGain();

// set gain default to 0 (no sound)
ampNode.gain.value = 0;


// connect the oscillator node to the gain node
oscNode.connect(ampNode);

// connect the gain node to audio output device on audio context
ampNode.connect(audioContext.destination);

// start the oscillator
oscNode.start();

// amplitude modulation variables
let modulationStartTime = 0;
// 1/24 Hz frequency 
const modulationFrequency = 1/24; 


// TOGGLE SOUND
// define a sound status var
let soundStatus = false;

// toggle sound function 
function toggleSound() {
  // if sound status value is off
  if (soundStatus === false) {
    // modulation start time
    modulationStartTime = audioContext.currentTime;

    // set the value to true
    soundStatus = true;

    // start amplitude modulation
    modulateAmplitude();

    // change background colour
    div.style.backgroundColor = 'deeppink';
  }
  // if sound status value is on
  else if (soundStatus === true) {

    // set the value to false
    soundStatus = false;
      
    // change background colour
    div.style.backgroundColor = 'pink';
  }
}

// anonymous function click handler 
div.onclick = () => {
  // if the audio context is still suspended
  // resume the audio context first
  if (audioContext.state != 'running') init_audio ();

  // then call the toggle sound function
  toggleSound();
}

// function to apply amplitude modulation
function modulateAmplitude() {
  // declaring time now when function called
  const now = audioContext.currentTime;
  // calculating elapsed time
  const elapsedTime = now - modulationStartTime;
  
  // modulation - creates values between 0 and 1
  // starting from the middle 0.5
  // sin produces a wave between -1 and 1
  // therefore * 0.5 to make it -0.5 to 0.5
  // +0.5 makes ot 0 to 1
  // angle = 2 * Math.PI * modulationFrequency * elapsedTime
  // 2 * Math.PI = full circle 
  // modulationFrequecy = 1/24 means we complete a cycle every 24 s
  // elapsedTime = time since we started oscillation 
  const modulation = 0.5 + 0.5 * Math.sin(2 * Math.PI * modulationFrequency * elapsedTime);
  
  // apply to gain (with base amplitude of 0.2)
  ampNode.gain.value = 0.2 * modulation;
  
  // next modulation update
  if (soundStatus === true) {
    requestAnimationFrame(modulateAmplitude);
  } else {
    ampNode.gain.value = 0;
  }
}
</script>

---

<script type="module">
   import codeBlockRenderer from "/scripts/codeblock_renderer.js"
   codeBlockRenderer (document, `script`, `sound`)
</script>
